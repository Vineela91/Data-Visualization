# -*- coding: utf-8 -*-
"""twitter_snsscrape_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1df1-Xbl3QoloKAhG9WZGcij0uDGucdyH
"""

!pip3 install snscrape
!pip3 install langdetect

import snscrape.modules.twitter as sntwitter
import pandas as pd
from langdetect import detect

search_term='leicester city council' # enter the search word

start_date='2015-01-01' # enter the start date for tweet search

end_date='2023-02-24' # enter end data for tweet search

num_tweets=20000 # enter the number of tweets to be scraped.

lst=[f'{search_term} since:{start_date} until:{end_date}']#, num_tweets] # first entry is the search keyword with start and end date. The second entry is the number of tweets

twt_data=[] # creating an empty list to append all the collected tweets
ct=0
for j,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{lst[0]}').get_items()): # enumerating through the tweets
 if ct <= num_tweets-1: # scrape tweets only if the number of tweets are less than the required number.
   lan=detect(tweet.content) # check the language of the tweet
   if lan=='en':
     twt_data.append([tweet.date, tweet.id, tweet.content, tweet.user.username])  # append the tweet if the language is in english
     ct=ct+1 # incrementing the count of number of tweets
 else:
   break
twt_df = pd.DataFrame(twt_data, columns=['Date_time', 'Tweet_id', 'Tweet_text', 'User_name']) # create pandas data frame of all the tweets

twt_df.head(20)

twt_df.info()

twt_df['Tweet_text']#.head(100)

twt_df.iloc[1,2]

twt_df.to_csv('leicester_city_twitter_data_20k.csv')